{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jI9WlAQ30jh5",
        "outputId": "3b1294a1-b012-41a5-a9b6-c1854e787daa"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 1: 套件安裝與設定\n",
        "# ==========================================\n",
        "\n",
        "# 安裝必要套件\n",
        "!pip install pycocotools\n",
        "!pip install albumentations\n",
        "!pip install efficientnet-pytorch\n",
        "!pip install timm\n",
        "!pip install sklearn\n",
        "!pip install matplotlib seaborn\n",
        "\n",
        "# 匯入基本套件\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# 深度學習套件\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import timm\n",
        "\n",
        "# 評估套件\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# COCO API\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# 設定裝置\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"使用裝置: {device}\")\n",
        "\n",
        "# 設定隨機種子\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "print(\"套件安裝與設定完成！\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B8-nUfho1fug",
        "outputId": "65b69b79-cdf6-4352-b9e2-8ec46ac2dce2"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 2: 資料下載與處理\n",
        "# ==========================================\n",
        "\n",
        "# 定義目標類別\n",
        "TARGET_CATEGORIES = ['bear', 'elephant', 'airplane', 'train']\n",
        "CATEGORY_TO_ID = {'bear': 23, 'elephant': 21, 'airplane': 5, 'train': 7}  # COCO category IDs\n",
        "ID_TO_LABEL = {0: 'bear', 1: 'elephant', 2: 'airplane', 3: 'train'}\n",
        "\n",
        "# 下載COCO annotations\n",
        "def download_coco_annotations():\n",
        "    \"\"\"下載COCO annotations檔案\"\"\"\n",
        "    annotations_dir = '/content/coco_annotations'\n",
        "    os.makedirs(annotations_dir, exist_ok=True)\n",
        "\n",
        "    # 下載train和val的annotations\n",
        "    urls = {\n",
        "        'train': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
        "    }\n",
        "\n",
        "    for split, url in urls.items():\n",
        "        zip_path = f'{annotations_dir}/annotations_trainval2017.zip'\n",
        "        if not os.path.exists(zip_path):\n",
        "            print(f\"下載 {split} annotations...\")\n",
        "            response = requests.get(url, stream=True)\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "                    f.write(chunk)\n",
        "\n",
        "            # 解壓縮\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(annotations_dir)\n",
        "\n",
        "    return annotations_dir\n",
        "\n",
        "# 下載並處理annotations\n",
        "annotations_dir = download_coco_annotations()\n",
        "\n",
        "# 載入COCO API\n",
        "train_ann_path = f\"{annotations_dir}/annotations/instances_train2017.json\"\n",
        "val_ann_path = f\"{annotations_dir}/annotations/instances_val2017.json\"\n",
        "\n",
        "coco_train = COCO(train_ann_path)\n",
        "coco_val = COCO(val_ann_path)\n",
        "\n",
        "def get_filtered_image_data(coco_api, split_name, max_images_per_class=2000):\n",
        "    \"\"\"獲取指定類別的圖片資料\"\"\"\n",
        "    print(f\"處理 {split_name} 資料...\")\n",
        "\n",
        "    # 獲取目標類別的category IDs\n",
        "    target_cat_ids = []\n",
        "    for cat_name in TARGET_CATEGORIES:\n",
        "        cat_id = CATEGORY_TO_ID[cat_name]\n",
        "        target_cat_ids.append(cat_id)\n",
        "\n",
        "    image_data = []\n",
        "\n",
        "    for idx, cat_name in enumerate(TARGET_CATEGORIES):\n",
        "        cat_id = CATEGORY_TO_ID[cat_name]\n",
        "        print(f\"處理類別: {cat_name} (ID: {cat_id})\")\n",
        "\n",
        "        # 獲取該類別的所有圖片ID\n",
        "        img_ids = coco_api.getImgIds(catIds=[cat_id])\n",
        "\n",
        "        # 限制圖片數量\n",
        "        if len(img_ids) > max_images_per_class:\n",
        "            img_ids = random.sample(img_ids, max_images_per_class)\n",
        "\n",
        "        print(f\"  找到 {len(img_ids)} 張圖片\")\n",
        "\n",
        "        for img_id in img_ids:\n",
        "            # 獲取圖片資訊\n",
        "            img_info = coco_api.loadImgs(img_id)[0]\n",
        "\n",
        "            # 獲取該圖片的所有annotations\n",
        "            ann_ids = coco_api.getAnnIds(imgIds=img_id, catIds=target_cat_ids)\n",
        "            anns = coco_api.loadAnns(ann_ids)\n",
        "\n",
        "            # 檢查是否包含目標類別\n",
        "            categories_in_image = set()\n",
        "            for ann in anns:\n",
        "                if ann['category_id'] in target_cat_ids:\n",
        "                    cat_name_found = next(name for name, id in CATEGORY_TO_ID.items() if id == ann['category_id'])\n",
        "                    categories_in_image.add(cat_name_found)\n",
        "\n",
        "            # 如果圖片主要包含當前處理的類別，則添加到資料集\n",
        "            if cat_name in categories_in_image:\n",
        "                image_data.append({\n",
        "                    'image_id': img_id,\n",
        "                    'file_name': img_info['file_name'],\n",
        "                    'label': idx,  # 重新編碼標籤 0,1,2,3\n",
        "                    'category_name': cat_name,\n",
        "                    'url': f\"http://images.cocodataset.org/{'train2017' if split_name == 'train' else 'val2017'}/{img_info['file_name']}\"\n",
        "                })\n",
        "\n",
        "    return image_data\n",
        "\n",
        "# 處理訓練和測試資料\n",
        "train_data_full = get_filtered_image_data(coco_train, 'train', max_images_per_class=500) \n",
        "test_data = get_filtered_image_data(coco_val, 'val', max_images_per_class=50)  \n",
        "\n",
        "# 將train集分割為 train (90%) 和 validation (10%)\n",
        "def split_train_validation(train_data, validation_ratio=0.1):\n",
        "    \"\"\"將訓練資料分割為訓練集和驗證集\"\"\"\n",
        "    print(\"分割訓練資料為 train (90%) 和 validation (10%)...\")\n",
        "\n",
        "    # 按類別分組\n",
        "    class_data = {}\n",
        "    for item in train_data:\n",
        "        label = item['label']\n",
        "        if label not in class_data:\n",
        "            class_data[label] = []\n",
        "        class_data[label].append(item)\n",
        "\n",
        "    train_final = []\n",
        "    val_data = []\n",
        "\n",
        "    # 每個類別單獨分割\n",
        "    for label, items in class_data.items():\n",
        "        random.shuffle(items)  # 隨機打亂\n",
        "        split_point = int(len(items) * validation_ratio)\n",
        "\n",
        "        val_data.extend(items[:split_point])  # 前10%作為驗證集\n",
        "        train_final.extend(items[split_point:])  # 後90%作為訓練集\n",
        "\n",
        "    return train_final, val_data\n",
        "\n",
        "# 分割資料集\n",
        "train_data, val_data = split_train_validation(train_data_full, validation_ratio=0.1)\n",
        "\n",
        "print(f\"\\n📊 資料集分割結果:\")\n",
        "print(f\"訓練資料 (train): {len(train_data)} 張圖片 (原train的90%)\")\n",
        "print(f\"驗證資料 (validation): {len(val_data)} 張圖片 (原train的10%)\")\n",
        "print(f\"測試資料 (test): {len(test_data)} 張圖片 (原val集)\")\n",
        "\n",
        "# 檢查各類別分布\n",
        "for split_name, data in [('Train', train_data), ('Validation', val_data), ('Test', test_data)]:\n",
        "    print(f\"\\n{split_name} 類別分布:\")\n",
        "    for i, cat_name in enumerate(TARGET_CATEGORIES):\n",
        "        count = sum(1 for item in data if item['label'] == i)\n",
        "        print(f\"  {cat_name}: {count} 張\")\n",
        "\n",
        "print(\"\\n資料處理完成！\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSIqIZbz5f_E",
        "outputId": "64f544f2-c3e1-4643-c356-eafce5443e3f"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 3: 資料集類別與資料載入\n",
        "# ==========================================\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    \"\"\"自定義COCO資料集類別\"\"\"\n",
        "\n",
        "    def __init__(self, data, transform=None, download_images=True):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.download_images = download_images\n",
        "        self.images_dir = '/content/coco_images'\n",
        "        os.makedirs(self.images_dir, exist_ok=True)\n",
        "\n",
        "        if self.download_images:\n",
        "            self._download_images()\n",
        "\n",
        "    def _download_images(self):\n",
        "        \"\"\"下載圖片到本地\"\"\"\n",
        "        print(\"開始下載圖片...\")\n",
        "\n",
        "        for item in tqdm(self.data):\n",
        "            img_path = os.path.join(self.images_dir, item['file_name'])\n",
        "            if not os.path.exists(img_path):\n",
        "                try:\n",
        "                    response = requests.get(item['url'], timeout=10)\n",
        "                    if response.status_code == 200:\n",
        "                        with open(img_path, 'wb') as f:\n",
        "                            f.write(response.content)\n",
        "                except Exception as e:\n",
        "                    print(f\"下載失敗 {item['file_name']}: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        img_path = os.path.join(self.images_dir, item['file_name'])\n",
        "\n",
        "        try:\n",
        "            # 載入圖片\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            label = item['label']\n",
        "\n",
        "            # 應用轉換\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"載入圖片錯誤 {item['file_name']}: {e}\")\n",
        "            # 返回一個空白圖片\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, item['label']\n",
        "\n",
        "# 定義資料轉換\n",
        "def get_transforms():\n",
        "    \"\"\"獲取訓練和驗證的資料轉換\"\"\"\n",
        "\n",
        "    # 訓練時的資料增強\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])  # ImageNet標準化\n",
        "    ])\n",
        "\n",
        "    # 驗證時的轉換\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# 創建轉換\n",
        "train_transform, val_transform = get_transforms()\n",
        "\n",
        "# 創建三個資料集\n",
        "print(\"創建訓練資料集...\")\n",
        "train_dataset = COCODataset(train_data, transform=train_transform, download_images=False)\n",
        "\n",
        "print(\"創建驗證資料集...\")\n",
        "val_dataset = COCODataset(val_data, transform=val_transform, download_images=False)\n",
        "\n",
        "print(\"創建測試資料集...\")\n",
        "test_dataset = COCODataset(test_data, transform=val_transform, download_images=False)\n",
        "\n",
        "# 創建資料載入器\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"\\n資料載入器設定完成:\")\n",
        "print(f\"訓練批次數: {len(train_loader)} (樣本數: {len(train_dataset)})\")\n",
        "print(f\"驗證批次數: {len(val_loader)} (樣本數: {len(val_dataset)})\")\n",
        "print(f\"測試批次數: {len(test_loader)} (樣本數: {len(test_dataset)})\")\n",
        "print(f\"批次大小: {batch_size}\")\n",
        "\n",
        "# 下載圖片 \n",
        "def download_images_in_batches(dataset, batch_size=100):\n",
        "    \"\"\"分批下載圖片\"\"\"\n",
        "    total_images = len(dataset.data)\n",
        "\n",
        "    for start_idx in range(0, total_images, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, total_images)\n",
        "        batch_data = dataset.data[start_idx:end_idx]\n",
        "\n",
        "        print(f\"下載第 {start_idx//batch_size + 1} 批圖片 ({start_idx+1}-{end_idx}/{total_images})\")\n",
        "\n",
        "        for item in tqdm(batch_data):\n",
        "            img_path = os.path.join(dataset.images_dir, item['file_name'])\n",
        "            if not os.path.exists(img_path):\n",
        "                try:\n",
        "                    response = requests.get(item['url'], timeout=10)\n",
        "                    if response.status_code == 200:\n",
        "                        with open(img_path, 'wb') as f:\n",
        "                            f.write(response.content)\n",
        "                except Exception as e:\n",
        "                    print(f\"下載失敗 {item['file_name']}: {e}\")\n",
        "\n",
        "# 開始下載圖片\n",
        "print(\"\\n開始下載訓練圖片...\")\n",
        "download_images_in_batches(train_dataset)\n",
        "\n",
        "print(\"\\n開始下載驗證圖片...\")\n",
        "download_images_in_batches(val_dataset)\n",
        "\n",
        "print(\"\\n開始下載測試圖片...\")\n",
        "download_images_in_batches(test_dataset)\n",
        "\n",
        "print(\"\\n資料集與資料載入器準備完成！\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mwx5rIv15pp3",
        "outputId": "1c7334e6-9a1f-4d7c-9dcf-6ce99930654b"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 4: EfficientNet模型定義\n",
        "# ==========================================\n",
        "\n",
        "class EfficientNetClassifier(nn.Module):\n",
        "    def __init__(self, model_name='efficientnet_b3', num_classes=4, pretrained=True, dropout_rate=0.3):\n",
        "        super(EfficientNetClassifier, self).__init__()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # 載入預訓練的EfficientNet\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0,  # 移除分類頭\n",
        "            global_pool=''  # 移除全域池化\n",
        "        )\n",
        "\n",
        "        # 獲取特徵維度\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.randn(1, 3, 224, 224)\n",
        "            features = self.backbone(dummy_input)\n",
        "            self.feature_dim = features.shape[1]\n",
        "\n",
        "        # 自定義分類頭\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # 全域平均池化\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(self.feature_dim, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(dropout_rate/2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # 初始化分類頭權重\n",
        "        self._initialize_classifier()\n",
        "\n",
        "    def _initialize_classifier(self):\n",
        "        \"\"\"初始化分類頭的權重\"\"\"\n",
        "        for m in self.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 特徵提取\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # 分類\n",
        "        output = self.classifier(features)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def freeze_backbone(self, freeze=True):\n",
        "        \"\"\"凍結或解凍backbone參數\"\"\"\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "\n",
        "    def unfreeze_last_layers(self, num_layers=2):\n",
        "        \"\"\"解凍backbone的最後幾層\"\"\"\n",
        "        # 獲取所有參數\n",
        "        all_params = list(self.backbone.named_parameters())\n",
        "\n",
        "        # 計算要解凍的參數數量\n",
        "        total_layers = len(all_params)\n",
        "        unfreeze_from = max(0, total_layers - num_layers)\n",
        "\n",
        "        # 解凍最後幾層\n",
        "        for i, (name, param) in enumerate(all_params):\n",
        "            if i >= unfreeze_from:\n",
        "                param.requires_grad = True\n",
        "                print(f\"解凍層: {name}\")\n",
        "\n",
        "# 創建模型\n",
        "def create_model(model_name='efficientnet_b3', num_classes=4, pretrained=True):\n",
        "    \"\"\"創建EfficientNet分類模型\"\"\"\n",
        "\n",
        "    print(f\"創建模型: {model_name}\")\n",
        "    print(f\"類別數量: {num_classes}\")\n",
        "    print(f\"使用預訓練權重: {pretrained}\")\n",
        "\n",
        "    model = EfficientNetClassifier(\n",
        "        model_name=model_name,\n",
        "        num_classes=num_classes,\n",
        "        pretrained=pretrained,\n",
        "        dropout_rate=0.3\n",
        "    )\n",
        "\n",
        "    # 將模型移到指定裝置\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 顯示模型資訊\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\n模型資訊:\")\n",
        "    print(f\"總參數量: {total_params:,}\")\n",
        "    print(f\"可訓練參數量: {trainable_params:,}\")\n",
        "    print(f\"特徵維度: {model.feature_dim}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# 創建模型實例\n",
        "model = create_model(\n",
        "    model_name='efficientnet_b3',  # 使用B3版本，平衡效能和效率\n",
        "    num_classes=4,\n",
        "    pretrained=True\n",
        ")\n",
        "\n",
        "print(f\"\\n模型架構:\")\n",
        "print(model)\n",
        "\n",
        "print(f\"\\nEfficientNet模型創建完成！\")\n",
        "print(f\"模型已移至裝置: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RYFcSU-_8qRR",
        "outputId": "13ad2bd4-66dc-43f3-ff39-d138d1f752f1"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 5: 訓練設定與函數\n",
        "# ==========================================\n",
        "\n",
        "# 損失函數和優化器設定\n",
        "def setup_training(model, learning_rate=0.001):\n",
        "    \"\"\"設定訓練相關的損失函數、優化器和學習率調度器\"\"\"\n",
        "\n",
        "    # 損失函數 - 使用交叉熵損失，並加入標籤平滑\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # 優化器 - 使用AdamW，加入權重衰減\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    # 學習率調度器 - 使用餘弦退火\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=10,  # 第一次重啟的週期\n",
        "        T_mult=2,  # 每次重啟後週期的倍數\n",
        "        eta_min=1e-6  # 最小學習率\n",
        "    )\n",
        "\n",
        "    return criterion, optimizer, scheduler\n",
        "\n",
        "# 訓練函數\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"訓練一個epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc='Training')\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        # 零梯度\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 前向傳播\n",
        "        with torch.cuda.amp.autocast():  # 使用混合精度訓練\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # 反向傳播\n",
        "        loss.backward()\n",
        "\n",
        "        # 梯度裁剪防止梯度爆炸\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # 優化器步進\n",
        "        optimizer.step()\n",
        "\n",
        "        # 統計\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "        # 更新進度條\n",
        "        current_acc = running_corrects.double() / total_samples\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{running_loss / total_samples:.4f}',\n",
        "            'Acc': f'{current_acc:.4f}'\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "# 驗證函數\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"驗證一個epoch\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc='Validation')\n",
        "\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # 前向傳播\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 統計\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "            # 收集預測結果用於混淆矩陣\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # 更新進度條\n",
        "            current_acc = running_corrects.double() / total_samples\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{running_loss / total_samples:.4f}',\n",
        "                'Acc': f'{current_acc:.4f}'\n",
        "            })\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "    return epoch_loss, epoch_acc.item(), all_preds, all_labels\n",
        "\n",
        "# 主訓練循環\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                num_epochs=50, patience=10, target_accuracy=0.95):\n",
        "    \"\"\"完整的訓練過程\"\"\"\n",
        "\n",
        "    print(f\"開始訓練，目標準確率: {target_accuracy:.1%}\")\n",
        "    print(f\"最大訓練輪數: {num_epochs}\")\n",
        "    print(f\"早停耐心值: {patience}\")\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = None\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # 訓練階段\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # 驗證階段\n",
        "        val_loss, val_acc, val_preds, val_labels = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        # 學習率調度\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # 記錄結果\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f'訓練 - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}')\n",
        "        print(f'驗證 - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}')\n",
        "        print(f'學習率: {current_lr:.6f}')\n",
        "\n",
        "        # 檢查是否達到目標準確率\n",
        "        if val_acc >= target_accuracy:\n",
        "            print(f'\\n🎉 達到目標準確率 {target_accuracy:.1%}！')\n",
        "            best_model_wts = model.state_dict().copy()\n",
        "            best_acc = val_acc\n",
        "            break\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "            print(f'💾 新的最佳驗證準確率: {best_acc:.4f}')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f'驗證準確率未提升 ({patience_counter}/{patience})')\n",
        "\n",
        "        # 早停檢查\n",
        "        if patience_counter >= patience:\n",
        "            print(f'\\n早停觸發！最佳驗證準確率: {best_acc:.4f}')\n",
        "            break\n",
        "\n",
        "    # 載入最佳模型權重\n",
        "    if best_model_wts is not None:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # 返回訓練歷史\n",
        "    history = {\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accs': val_accs,\n",
        "        'best_acc': best_acc\n",
        "    }\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# 設定訓練參數\n",
        "criterion, optimizer, scheduler = setup_training(model, learning_rate=0.001)\n",
        "\n",
        "print(\"訓練設定完成！\")\n",
        "print(f\"損失函數: {criterion}\")\n",
        "print(f\"優化器: {optimizer}\")\n",
        "print(f\"學習率調度器: {scheduler}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNPp_vT48xes",
        "outputId": "b843099c-28bb-443f-919f-a30a4ed27edd"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 6: 模型訓練執行\n",
        "# ==========================================\n",
        "\n",
        "# 開始訓練模型\n",
        "print(\"開始訓練模型...\")\n",
        "print(f\"目標類別: {TARGET_CATEGORIES}\")\n",
        "print(f\"訓練樣本數: {len(train_data)}\")\n",
        "print(f\"驗證樣本數: {len(val_data)}\")\n",
        "print(f\"使用裝置: {device}\")\n",
        "\n",
        "# 執行訓練\n",
        "trained_model, training_history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=30,  # 最大訓練輪數\n",
        "    patience=10,    # 早停耐心值\n",
        "    target_accuracy=0.95  # 目標準確率 95%\n",
        ")\n",
        "\n",
        "print(f\"\\n🎯 訓練完成！\")\n",
        "print(f\"最佳驗證準確率: {training_history['best_acc']:.4f}\")\n",
        "\n",
        "# 保存模型\n",
        "model_save_path = '/content/best_efficientnet_model.pth'\n",
        "torch.save({\n",
        "    'model_state_dict': trained_model.state_dict(),\n",
        "    'model_architecture': 'efficientnet_b3',\n",
        "    'num_classes': 4,\n",
        "    'target_categories': TARGET_CATEGORIES,\n",
        "    'training_history': training_history,\n",
        "    'feature_dim': trained_model.feature_dim\n",
        "}, model_save_path)\n",
        "\n",
        "print(f\"模型已保存至: {model_save_path}\")\n",
        "\n",
        "# 顯示訓練歷史摘要\n",
        "print(f\"\\n📊 訓練歷史摘要:\")\n",
        "print(f\"訓練輪數: {len(training_history['train_losses'])}\")\n",
        "print(f\"最終訓練準確率: {training_history['train_accs'][-1]:.4f}\")\n",
        "print(f\"最終驗證準確率: {training_history['val_accs'][-1]:.4f}\")\n",
        "print(f\"最佳驗證準確率: {training_history['best_acc']:.4f}\")\n",
        "\n",
        "# 如果需要，可以進行微調訓練\n",
        "def fine_tune_training(model, train_loader, val_loader, target_accuracy=0.95):\n",
        "    \"\"\"微調訓練 - 解凍更多層進行精細調整\"\"\"\n",
        "    print(\"\\n🔧 開始微調訓練...\")\n",
        "\n",
        "    # 解凍backbone的最後幾層\n",
        "    model.unfreeze_last_layers(num_layers=10)\n",
        "\n",
        "    # 降低學習率進行微調\n",
        "    fine_tune_optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=0.0001,  # 更小的學習率\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    fine_tune_scheduler = optim.lr_scheduler.StepLR(\n",
        "        fine_tune_optimizer,\n",
        "        step_size=5,\n",
        "        gamma=0.5\n",
        "    )\n",
        "\n",
        "    # 微調訓練\n",
        "    fine_tuned_model, fine_tune_history = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=fine_tune_optimizer,\n",
        "        scheduler=fine_tune_scheduler,\n",
        "        num_epochs=20,  # 較少的訓練輪數\n",
        "        patience=5,     # 較小的耐心值\n",
        "        target_accuracy=target_accuracy\n",
        "    )\n",
        "\n",
        "    return fine_tuned_model, fine_tune_history\n",
        "\n",
        "# 檢查是否需要微調\n",
        "# if training_history['best_acc'] < 0.95:\n",
        "#     print(f\"\\n當前最佳準確率 {training_history['best_acc']:.4f} < 95%，開始微調...\")\n",
        "#     trained_model, fine_tune_history = fine_tune_training(\n",
        "#         trained_model, train_loader, val_loader, target_accuracy=0.95\n",
        "#     )\n",
        "\n",
        "#     # 更新訓練歷史\n",
        "#     training_history['fine_tune_history'] = fine_tune_history\n",
        "#     training_history['final_best_acc'] = fine_tune_history['best_acc']\n",
        "\n",
        "#     # 重新保存模型\n",
        "#     torch.save({\n",
        "#         'model_state_dict': trained_model.state_dict(),\n",
        "#         'model_architecture': 'efficientnet_b3',\n",
        "#         'num_classes': 4,\n",
        "#         'target_categories': TARGET_CATEGORIES,\n",
        "#         'training_history': training_history,\n",
        "#         'feature_dim': trained_model.feature_dim\n",
        "#     }, model_save_path)\n",
        "\n",
        "#     print(f\"微調後最佳準確率: {fine_tune_history['best_acc']:.4f}\")\n",
        "\n",
        "# print(\"\\n✅ 模型訓練階段完成！\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dD_tESnk-C5O",
        "outputId": "dda91e7f-cbaf-43ce-c417-860f063024f9"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 7: 模型評估與混淆矩陣\n",
        "# ==========================================\n",
        "\n",
        "def evaluate_model(model, data_loader, device, class_names):\n",
        "    \"\"\"\n",
        "    計算整體準確率、各類別準確率和混淆矩陣\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    print(\"正在進行模型評估...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc='評估中'):\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # 前向傳播\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # 收集結果\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # 轉換為numpy陣列\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    return all_preds, all_labels, all_probs\n",
        "\n",
        "def plot_confusion_matrices(y_true, y_pred, class_names):\n",
        "    \"\"\"\n",
        "    繪製每個類別的混淆矩陣和整體混淆矩陣\n",
        "    \"\"\"\n",
        "    print(\"\\n📊 生成混淆矩陣...\")\n",
        "\n",
        "    # 整體混淆矩陣\n",
        "    overall_cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # 設定圖表樣式\n",
        "    plt.style.use('default')\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('COCO 4-Class Classification - Confusion Matrices', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 顏色映射\n",
        "    colors = ['Blues', 'Greens', 'Oranges', 'Reds', 'Purples']\n",
        "\n",
        "    # 繪製每個類別的混淆矩陣\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        row = i // 3\n",
        "        col = i % 3\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        # 為當前類別創建二元混淆矩陣\n",
        "        # True Positive, False Negative, False Positive, True Negative\n",
        "        y_true_binary = (y_true == i).astype(int)\n",
        "        y_pred_binary = (y_pred == i).astype(int)\n",
        "\n",
        "        binary_cm = confusion_matrix(y_true_binary, y_pred_binary)\n",
        "\n",
        "        # 繪製熱圖\n",
        "        sns.heatmap(binary_cm,\n",
        "                   annot=True,\n",
        "                   fmt='d',\n",
        "                   cmap=colors[i],\n",
        "                   xticklabels=[f'Not {class_name}', f'{class_name}'],\n",
        "                   yticklabels=[f'Not {class_name}', f'{class_name}'],\n",
        "                   ax=ax,\n",
        "                   cbar_kws={'shrink': 0.8})\n",
        "\n",
        "        # 計算該類別的精確率、召回率和F1分數\n",
        "        TP = binary_cm[1, 1]\n",
        "        FN = binary_cm[1, 0]\n",
        "        FP = binary_cm[0, 1]\n",
        "        TN = binary_cm[0, 0]\n",
        "\n",
        "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "        ax.set_title(f'{class_name}\\nAcc: {accuracy:.3f} | P: {precision:.3f} | R: {recall:.3f} | F1: {f1:.3f}')\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('Actual')\n",
        "\n",
        "    # 繪製整體混淆矩陣\n",
        "    ax_overall = axes[1, 1]\n",
        "    sns.heatmap(overall_cm,\n",
        "               annot=True,\n",
        "               fmt='d',\n",
        "               cmap='viridis',\n",
        "               xticklabels=class_names,\n",
        "               yticklabels=class_names,\n",
        "               ax=ax_overall,\n",
        "               cbar_kws={'shrink': 0.8})\n",
        "\n",
        "    ax_overall.set_title('Overall Confusion Matrix')\n",
        "    ax_overall.set_xlabel('Predicted')\n",
        "    ax_overall.set_ylabel('Actual')\n",
        "\n",
        "    # 隱藏最後一個子圖\n",
        "    axes[1, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return overall_cm\n",
        "\n",
        "def calculate_detailed_metrics(y_true, y_pred, y_probs, class_names):\n",
        "    \"\"\"\n",
        "    計算詳細的評估指標\n",
        "    \"\"\"\n",
        "    print(\"\\n📈 計算詳細評估指標...\")\n",
        "\n",
        "    # 整體準確率\n",
        "    overall_accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # 分類報告\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "    print(f\"\\n🎯 整體準確率: {overall_accuracy:.4f} ({overall_accuracy:.1%})\")\n",
        "    print(\"\\n📋 各類別詳細指標:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'類別':<12} {'精確率':<10} {'召回率':<10} {'F1分數':<10} {'準確率':<10} {'支持數':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 計算各類別的準確率\n",
        "    class_accuracies = []\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        y_true_binary = (y_true == i).astype(int)\n",
        "        y_pred_binary = (y_pred == i).astype(int)\n",
        "\n",
        "        # 計算混淆矩陣元素\n",
        "        TP = np.sum((y_true_binary == 1) & (y_pred_binary == 1))\n",
        "        TN = np.sum((y_true_binary == 0) & (y_pred_binary == 0))\n",
        "        FP = np.sum((y_true_binary == 0) & (y_pred_binary == 1))\n",
        "        FN = np.sum((y_true_binary == 1) & (y_pred_binary == 0))\n",
        "\n",
        "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "        class_accuracies.append(accuracy)\n",
        "\n",
        "        precision = report[class_name]['precision']\n",
        "        recall = report[class_name]['recall']\n",
        "        f1 = report[class_name]['f1-score']\n",
        "        support = report[class_name]['support']\n",
        "\n",
        "        print(f\"{class_name:<12} {precision:<10.3f} {recall:<10.3f} {f1:<10.3f} {accuracy:<10.3f} {support:<10.0f}\")\n",
        "\n",
        "        # 檢查是否達到95%準確率\n",
        "        if accuracy >= 0.95:\n",
        "            print(f\"  ✅ {class_name} 達到95%準確率目標！\")\n",
        "        else:\n",
        "            print(f\"  ❌ {class_name} 未達到95%準確率目標 (目前: {accuracy:.1%})\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 統計達到95%準確率的類別數\n",
        "    classes_above_95 = sum(1 for acc in class_accuracies if acc >= 0.95)\n",
        "\n",
        "    print(f\"\\n📊 達到95%準確率的類別數: {classes_above_95}/{len(class_names)}\")\n",
        "\n",
        "    if classes_above_95 == len(class_names):\n",
        "        print(\"🎉 所有類別都達到了95%準確率目標！\")\n",
        "    else:\n",
        "        print(f\"⚠️  還有 {len(class_names) - classes_above_95} 個類別未達到95%準確率目標\")\n",
        "\n",
        "    return {\n",
        "        'overall_accuracy': overall_accuracy,\n",
        "        'class_accuracies': dict(zip(class_names, class_accuracies)),\n",
        "        'classification_report': report,\n",
        "        'classes_above_95_percent': classes_above_95\n",
        "    }\n",
        "\n",
        "# 執行模型評估\n",
        "print(\"開始評估訓練好的模型...\")\n",
        "\n",
        "# 在驗證集上評估\n",
        "test_preds, test_labels, test_probs = evaluate_model(trained_model, test_loader, device, TARGET_CATEGORIES)\n",
        "\n",
        "# 繪製混淆矩陣\n",
        "overall_cm = plot_confusion_matrices(test_labels, test_preds, TARGET_CATEGORIES)\n",
        "\n",
        "# 計算詳細指標\n",
        "detailed_metrics = calculate_detailed_metrics(test_labels, test_preds, test_probs, TARGET_CATEGORIES)\n",
        "\n",
        "# 保存評估結果\n",
        "evaluation_results = {\n",
        "    'predictions': test_preds.tolist(),\n",
        "    'true_labels': test_labels.tolist(),\n",
        "    'probabilities': test_probs.tolist(),\n",
        "    'confusion_matrix': overall_cm.tolist(),\n",
        "    'detailed_metrics': detailed_metrics,\n",
        "    'class_names': TARGET_CATEGORIES\n",
        "}\n",
        "\n",
        "# 保存到文件\n",
        "import json\n",
        "with open('/content/evaluation_results.json', 'w') as f:\n",
        "    json.dump(evaluation_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n💾 評估結果已保存至: /content/evaluation_results.json\")\n",
        "print(\"\\n✅ 模型評估完成！\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
