{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jI9WlAQ30jh5",
        "outputId": "3b1294a1-b012-41a5-a9b6-c1854e787daa"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 1: å¥—ä»¶å®‰è£èˆ‡è¨­å®š\n",
        "# ==========================================\n",
        "\n",
        "# å®‰è£å¿…è¦å¥—ä»¶\n",
        "!pip install pycocotools\n",
        "!pip install albumentations\n",
        "!pip install efficientnet-pytorch\n",
        "!pip install timm\n",
        "!pip install sklearn\n",
        "!pip install matplotlib seaborn\n",
        "\n",
        "# åŒ¯å…¥åŸºæœ¬å¥—ä»¶\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# æ·±åº¦å­¸ç¿’å¥—ä»¶\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import timm\n",
        "\n",
        "# è©•ä¼°å¥—ä»¶\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# COCO API\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# è¨­å®šè£ç½®\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ä½¿ç”¨è£ç½®: {device}\")\n",
        "\n",
        "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "print(\"å¥—ä»¶å®‰è£èˆ‡è¨­å®šå®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B8-nUfho1fug",
        "outputId": "65b69b79-cdf6-4352-b9e2-8ec46ac2dce2"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 2: è³‡æ–™ä¸‹è¼‰èˆ‡è™•ç†\n",
        "# ==========================================\n",
        "\n",
        "# å®šç¾©ç›®æ¨™é¡åˆ¥\n",
        "TARGET_CATEGORIES = ['bear', 'elephant', 'airplane', 'train']\n",
        "CATEGORY_TO_ID = {'bear': 23, 'elephant': 21, 'airplane': 5, 'train': 7}  # COCO category IDs\n",
        "ID_TO_LABEL = {0: 'bear', 1: 'elephant', 2: 'airplane', 3: 'train'}\n",
        "\n",
        "# ä¸‹è¼‰COCO annotations\n",
        "def download_coco_annotations():\n",
        "    \"\"\"ä¸‹è¼‰COCO annotationsæª”æ¡ˆ\"\"\"\n",
        "    annotations_dir = '/content/coco_annotations'\n",
        "    os.makedirs(annotations_dir, exist_ok=True)\n",
        "\n",
        "    # ä¸‹è¼‰trainå’Œvalçš„annotations\n",
        "    urls = {\n",
        "        'train': 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
        "    }\n",
        "\n",
        "    for split, url in urls.items():\n",
        "        zip_path = f'{annotations_dir}/annotations_trainval2017.zip'\n",
        "        if not os.path.exists(zip_path):\n",
        "            print(f\"ä¸‹è¼‰ {split} annotations...\")\n",
        "            response = requests.get(url, stream=True)\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "                    f.write(chunk)\n",
        "\n",
        "            # è§£å£“ç¸®\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(annotations_dir)\n",
        "\n",
        "    return annotations_dir\n",
        "\n",
        "# ä¸‹è¼‰ä¸¦è™•ç†annotations\n",
        "annotations_dir = download_coco_annotations()\n",
        "\n",
        "# è¼‰å…¥COCO API\n",
        "train_ann_path = f\"{annotations_dir}/annotations/instances_train2017.json\"\n",
        "val_ann_path = f\"{annotations_dir}/annotations/instances_val2017.json\"\n",
        "\n",
        "coco_train = COCO(train_ann_path)\n",
        "coco_val = COCO(val_ann_path)\n",
        "\n",
        "def get_filtered_image_data(coco_api, split_name, max_images_per_class=2000):\n",
        "    \"\"\"ç²å–æŒ‡å®šé¡åˆ¥çš„åœ–ç‰‡è³‡æ–™\"\"\"\n",
        "    print(f\"è™•ç† {split_name} è³‡æ–™...\")\n",
        "\n",
        "    # ç²å–ç›®æ¨™é¡åˆ¥çš„category IDs\n",
        "    target_cat_ids = []\n",
        "    for cat_name in TARGET_CATEGORIES:\n",
        "        cat_id = CATEGORY_TO_ID[cat_name]\n",
        "        target_cat_ids.append(cat_id)\n",
        "\n",
        "    image_data = []\n",
        "\n",
        "    for idx, cat_name in enumerate(TARGET_CATEGORIES):\n",
        "        cat_id = CATEGORY_TO_ID[cat_name]\n",
        "        print(f\"è™•ç†é¡åˆ¥: {cat_name} (ID: {cat_id})\")\n",
        "\n",
        "        # ç²å–è©²é¡åˆ¥çš„æ‰€æœ‰åœ–ç‰‡ID\n",
        "        img_ids = coco_api.getImgIds(catIds=[cat_id])\n",
        "\n",
        "        # é™åˆ¶åœ–ç‰‡æ•¸é‡\n",
        "        if len(img_ids) > max_images_per_class:\n",
        "            img_ids = random.sample(img_ids, max_images_per_class)\n",
        "\n",
        "        print(f\"  æ‰¾åˆ° {len(img_ids)} å¼µåœ–ç‰‡\")\n",
        "\n",
        "        for img_id in img_ids:\n",
        "            # ç²å–åœ–ç‰‡è³‡è¨Š\n",
        "            img_info = coco_api.loadImgs(img_id)[0]\n",
        "\n",
        "            # ç²å–è©²åœ–ç‰‡çš„æ‰€æœ‰annotations\n",
        "            ann_ids = coco_api.getAnnIds(imgIds=img_id, catIds=target_cat_ids)\n",
        "            anns = coco_api.loadAnns(ann_ids)\n",
        "\n",
        "            # æª¢æŸ¥æ˜¯å¦åŒ…å«ç›®æ¨™é¡åˆ¥\n",
        "            categories_in_image = set()\n",
        "            for ann in anns:\n",
        "                if ann['category_id'] in target_cat_ids:\n",
        "                    cat_name_found = next(name for name, id in CATEGORY_TO_ID.items() if id == ann['category_id'])\n",
        "                    categories_in_image.add(cat_name_found)\n",
        "\n",
        "            # å¦‚æœåœ–ç‰‡ä¸»è¦åŒ…å«ç•¶å‰è™•ç†çš„é¡åˆ¥ï¼Œå‰‡æ·»åŠ åˆ°è³‡æ–™é›†\n",
        "            if cat_name in categories_in_image:\n",
        "                image_data.append({\n",
        "                    'image_id': img_id,\n",
        "                    'file_name': img_info['file_name'],\n",
        "                    'label': idx,  # é‡æ–°ç·¨ç¢¼æ¨™ç±¤ 0,1,2,3\n",
        "                    'category_name': cat_name,\n",
        "                    'url': f\"http://images.cocodataset.org/{'train2017' if split_name == 'train' else 'val2017'}/{img_info['file_name']}\"\n",
        "                })\n",
        "\n",
        "    return image_data\n",
        "\n",
        "# è™•ç†è¨“ç·´å’Œæ¸¬è©¦è³‡æ–™\n",
        "train_data_full = get_filtered_image_data(coco_train, 'train', max_images_per_class=500) \n",
        "test_data = get_filtered_image_data(coco_val, 'val', max_images_per_class=50)  \n",
        "\n",
        "# å°‡trainé›†åˆ†å‰²ç‚º train (90%) å’Œ validation (10%)\n",
        "def split_train_validation(train_data, validation_ratio=0.1):\n",
        "    \"\"\"å°‡è¨“ç·´è³‡æ–™åˆ†å‰²ç‚ºè¨“ç·´é›†å’Œé©—è­‰é›†\"\"\"\n",
        "    print(\"åˆ†å‰²è¨“ç·´è³‡æ–™ç‚º train (90%) å’Œ validation (10%)...\")\n",
        "\n",
        "    # æŒ‰é¡åˆ¥åˆ†çµ„\n",
        "    class_data = {}\n",
        "    for item in train_data:\n",
        "        label = item['label']\n",
        "        if label not in class_data:\n",
        "            class_data[label] = []\n",
        "        class_data[label].append(item)\n",
        "\n",
        "    train_final = []\n",
        "    val_data = []\n",
        "\n",
        "    # æ¯å€‹é¡åˆ¥å–®ç¨åˆ†å‰²\n",
        "    for label, items in class_data.items():\n",
        "        random.shuffle(items)  # éš¨æ©Ÿæ‰“äº‚\n",
        "        split_point = int(len(items) * validation_ratio)\n",
        "\n",
        "        val_data.extend(items[:split_point])  # å‰10%ä½œç‚ºé©—è­‰é›†\n",
        "        train_final.extend(items[split_point:])  # å¾Œ90%ä½œç‚ºè¨“ç·´é›†\n",
        "\n",
        "    return train_final, val_data\n",
        "\n",
        "# åˆ†å‰²è³‡æ–™é›†\n",
        "train_data, val_data = split_train_validation(train_data_full, validation_ratio=0.1)\n",
        "\n",
        "print(f\"\\nğŸ“Š è³‡æ–™é›†åˆ†å‰²çµæœ:\")\n",
        "print(f\"è¨“ç·´è³‡æ–™ (train): {len(train_data)} å¼µåœ–ç‰‡ (åŸtrainçš„90%)\")\n",
        "print(f\"é©—è­‰è³‡æ–™ (validation): {len(val_data)} å¼µåœ–ç‰‡ (åŸtrainçš„10%)\")\n",
        "print(f\"æ¸¬è©¦è³‡æ–™ (test): {len(test_data)} å¼µåœ–ç‰‡ (åŸvalé›†)\")\n",
        "\n",
        "# æª¢æŸ¥å„é¡åˆ¥åˆ†å¸ƒ\n",
        "for split_name, data in [('Train', train_data), ('Validation', val_data), ('Test', test_data)]:\n",
        "    print(f\"\\n{split_name} é¡åˆ¥åˆ†å¸ƒ:\")\n",
        "    for i, cat_name in enumerate(TARGET_CATEGORIES):\n",
        "        count = sum(1 for item in data if item['label'] == i)\n",
        "        print(f\"  {cat_name}: {count} å¼µ\")\n",
        "\n",
        "print(\"\\nè³‡æ–™è™•ç†å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSIqIZbz5f_E",
        "outputId": "64f544f2-c3e1-4643-c356-eafce5443e3f"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 3: è³‡æ–™é›†é¡åˆ¥èˆ‡è³‡æ–™è¼‰å…¥\n",
        "# ==========================================\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    \"\"\"è‡ªå®šç¾©COCOè³‡æ–™é›†é¡åˆ¥\"\"\"\n",
        "\n",
        "    def __init__(self, data, transform=None, download_images=True):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.download_images = download_images\n",
        "        self.images_dir = '/content/coco_images'\n",
        "        os.makedirs(self.images_dir, exist_ok=True)\n",
        "\n",
        "        if self.download_images:\n",
        "            self._download_images()\n",
        "\n",
        "    def _download_images(self):\n",
        "        \"\"\"ä¸‹è¼‰åœ–ç‰‡åˆ°æœ¬åœ°\"\"\"\n",
        "        print(\"é–‹å§‹ä¸‹è¼‰åœ–ç‰‡...\")\n",
        "\n",
        "        for item in tqdm(self.data):\n",
        "            img_path = os.path.join(self.images_dir, item['file_name'])\n",
        "            if not os.path.exists(img_path):\n",
        "                try:\n",
        "                    response = requests.get(item['url'], timeout=10)\n",
        "                    if response.status_code == 200:\n",
        "                        with open(img_path, 'wb') as f:\n",
        "                            f.write(response.content)\n",
        "                except Exception as e:\n",
        "                    print(f\"ä¸‹è¼‰å¤±æ•— {item['file_name']}: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        img_path = os.path.join(self.images_dir, item['file_name'])\n",
        "\n",
        "        try:\n",
        "            # è¼‰å…¥åœ–ç‰‡\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            label = item['label']\n",
        "\n",
        "            # æ‡‰ç”¨è½‰æ›\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"è¼‰å…¥åœ–ç‰‡éŒ¯èª¤ {item['file_name']}: {e}\")\n",
        "            # è¿”å›ä¸€å€‹ç©ºç™½åœ–ç‰‡\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, item['label']\n",
        "\n",
        "# å®šç¾©è³‡æ–™è½‰æ›\n",
        "def get_transforms():\n",
        "    \"\"\"ç²å–è¨“ç·´å’Œé©—è­‰çš„è³‡æ–™è½‰æ›\"\"\"\n",
        "\n",
        "    # è¨“ç·´æ™‚çš„è³‡æ–™å¢å¼·\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])  # ImageNetæ¨™æº–åŒ–\n",
        "    ])\n",
        "\n",
        "    # é©—è­‰æ™‚çš„è½‰æ›\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# å‰µå»ºè½‰æ›\n",
        "train_transform, val_transform = get_transforms()\n",
        "\n",
        "# å‰µå»ºä¸‰å€‹è³‡æ–™é›†\n",
        "print(\"å‰µå»ºè¨“ç·´è³‡æ–™é›†...\")\n",
        "train_dataset = COCODataset(train_data, transform=train_transform, download_images=False)\n",
        "\n",
        "print(\"å‰µå»ºé©—è­‰è³‡æ–™é›†...\")\n",
        "val_dataset = COCODataset(val_data, transform=val_transform, download_images=False)\n",
        "\n",
        "print(\"å‰µå»ºæ¸¬è©¦è³‡æ–™é›†...\")\n",
        "test_dataset = COCODataset(test_data, transform=val_transform, download_images=False)\n",
        "\n",
        "# å‰µå»ºè³‡æ–™è¼‰å…¥å™¨\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"\\nè³‡æ–™è¼‰å…¥å™¨è¨­å®šå®Œæˆ:\")\n",
        "print(f\"è¨“ç·´æ‰¹æ¬¡æ•¸: {len(train_loader)} (æ¨£æœ¬æ•¸: {len(train_dataset)})\")\n",
        "print(f\"é©—è­‰æ‰¹æ¬¡æ•¸: {len(val_loader)} (æ¨£æœ¬æ•¸: {len(val_dataset)})\")\n",
        "print(f\"æ¸¬è©¦æ‰¹æ¬¡æ•¸: {len(test_loader)} (æ¨£æœ¬æ•¸: {len(test_dataset)})\")\n",
        "print(f\"æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
        "\n",
        "# ä¸‹è¼‰åœ–ç‰‡ \n",
        "def download_images_in_batches(dataset, batch_size=100):\n",
        "    \"\"\"åˆ†æ‰¹ä¸‹è¼‰åœ–ç‰‡\"\"\"\n",
        "    total_images = len(dataset.data)\n",
        "\n",
        "    for start_idx in range(0, total_images, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, total_images)\n",
        "        batch_data = dataset.data[start_idx:end_idx]\n",
        "\n",
        "        print(f\"ä¸‹è¼‰ç¬¬ {start_idx//batch_size + 1} æ‰¹åœ–ç‰‡ ({start_idx+1}-{end_idx}/{total_images})\")\n",
        "\n",
        "        for item in tqdm(batch_data):\n",
        "            img_path = os.path.join(dataset.images_dir, item['file_name'])\n",
        "            if not os.path.exists(img_path):\n",
        "                try:\n",
        "                    response = requests.get(item['url'], timeout=10)\n",
        "                    if response.status_code == 200:\n",
        "                        with open(img_path, 'wb') as f:\n",
        "                            f.write(response.content)\n",
        "                except Exception as e:\n",
        "                    print(f\"ä¸‹è¼‰å¤±æ•— {item['file_name']}: {e}\")\n",
        "\n",
        "# é–‹å§‹ä¸‹è¼‰åœ–ç‰‡\n",
        "print(\"\\né–‹å§‹ä¸‹è¼‰è¨“ç·´åœ–ç‰‡...\")\n",
        "download_images_in_batches(train_dataset)\n",
        "\n",
        "print(\"\\né–‹å§‹ä¸‹è¼‰é©—è­‰åœ–ç‰‡...\")\n",
        "download_images_in_batches(val_dataset)\n",
        "\n",
        "print(\"\\né–‹å§‹ä¸‹è¼‰æ¸¬è©¦åœ–ç‰‡...\")\n",
        "download_images_in_batches(test_dataset)\n",
        "\n",
        "print(\"\\nè³‡æ–™é›†èˆ‡è³‡æ–™è¼‰å…¥å™¨æº–å‚™å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mwx5rIv15pp3",
        "outputId": "1c7334e6-9a1f-4d7c-9dcf-6ce99930654b"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 4: EfficientNetæ¨¡å‹å®šç¾©\n",
        "# ==========================================\n",
        "\n",
        "class EfficientNetClassifier(nn.Module):\n",
        "    def __init__(self, model_name='efficientnet_b3', num_classes=4, pretrained=True, dropout_rate=0.3):\n",
        "        super(EfficientNetClassifier, self).__init__()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # è¼‰å…¥é è¨“ç·´çš„EfficientNet\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0,  # ç§»é™¤åˆ†é¡é ­\n",
        "            global_pool=''  # ç§»é™¤å…¨åŸŸæ± åŒ–\n",
        "        )\n",
        "\n",
        "        # ç²å–ç‰¹å¾µç¶­åº¦\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.randn(1, 3, 224, 224)\n",
        "            features = self.backbone(dummy_input)\n",
        "            self.feature_dim = features.shape[1]\n",
        "\n",
        "        # è‡ªå®šç¾©åˆ†é¡é ­\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # å…¨åŸŸå¹³å‡æ± åŒ–\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(self.feature_dim, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(dropout_rate/2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # åˆå§‹åŒ–åˆ†é¡é ­æ¬Šé‡\n",
        "        self._initialize_classifier()\n",
        "\n",
        "    def _initialize_classifier(self):\n",
        "        \"\"\"åˆå§‹åŒ–åˆ†é¡é ­çš„æ¬Šé‡\"\"\"\n",
        "        for m in self.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ç‰¹å¾µæå–\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # åˆ†é¡\n",
        "        output = self.classifier(features)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def freeze_backbone(self, freeze=True):\n",
        "        \"\"\"å‡çµæˆ–è§£å‡backboneåƒæ•¸\"\"\"\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "\n",
        "    def unfreeze_last_layers(self, num_layers=2):\n",
        "        \"\"\"è§£å‡backboneçš„æœ€å¾Œå¹¾å±¤\"\"\"\n",
        "        # ç²å–æ‰€æœ‰åƒæ•¸\n",
        "        all_params = list(self.backbone.named_parameters())\n",
        "\n",
        "        # è¨ˆç®—è¦è§£å‡çš„åƒæ•¸æ•¸é‡\n",
        "        total_layers = len(all_params)\n",
        "        unfreeze_from = max(0, total_layers - num_layers)\n",
        "\n",
        "        # è§£å‡æœ€å¾Œå¹¾å±¤\n",
        "        for i, (name, param) in enumerate(all_params):\n",
        "            if i >= unfreeze_from:\n",
        "                param.requires_grad = True\n",
        "                print(f\"è§£å‡å±¤: {name}\")\n",
        "\n",
        "# å‰µå»ºæ¨¡å‹\n",
        "def create_model(model_name='efficientnet_b3', num_classes=4, pretrained=True):\n",
        "    \"\"\"å‰µå»ºEfficientNetåˆ†é¡æ¨¡å‹\"\"\"\n",
        "\n",
        "    print(f\"å‰µå»ºæ¨¡å‹: {model_name}\")\n",
        "    print(f\"é¡åˆ¥æ•¸é‡: {num_classes}\")\n",
        "    print(f\"ä½¿ç”¨é è¨“ç·´æ¬Šé‡: {pretrained}\")\n",
        "\n",
        "    model = EfficientNetClassifier(\n",
        "        model_name=model_name,\n",
        "        num_classes=num_classes,\n",
        "        pretrained=pretrained,\n",
        "        dropout_rate=0.3\n",
        "    )\n",
        "\n",
        "    # å°‡æ¨¡å‹ç§»åˆ°æŒ‡å®šè£ç½®\n",
        "    model = model.to(device)\n",
        "\n",
        "    # é¡¯ç¤ºæ¨¡å‹è³‡è¨Š\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\næ¨¡å‹è³‡è¨Š:\")\n",
        "    print(f\"ç¸½åƒæ•¸é‡: {total_params:,}\")\n",
        "    print(f\"å¯è¨“ç·´åƒæ•¸é‡: {trainable_params:,}\")\n",
        "    print(f\"ç‰¹å¾µç¶­åº¦: {model.feature_dim}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# å‰µå»ºæ¨¡å‹å¯¦ä¾‹\n",
        "model = create_model(\n",
        "    model_name='efficientnet_b3',  # ä½¿ç”¨B3ç‰ˆæœ¬ï¼Œå¹³è¡¡æ•ˆèƒ½å’Œæ•ˆç‡\n",
        "    num_classes=4,\n",
        "    pretrained=True\n",
        ")\n",
        "\n",
        "print(f\"\\næ¨¡å‹æ¶æ§‹:\")\n",
        "print(model)\n",
        "\n",
        "print(f\"\\nEfficientNetæ¨¡å‹å‰µå»ºå®Œæˆï¼\")\n",
        "print(f\"æ¨¡å‹å·²ç§»è‡³è£ç½®: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RYFcSU-_8qRR",
        "outputId": "13ad2bd4-66dc-43f3-ff39-d138d1f752f1"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 5: è¨“ç·´è¨­å®šèˆ‡å‡½æ•¸\n",
        "# ==========================================\n",
        "\n",
        "# æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨è¨­å®š\n",
        "def setup_training(model, learning_rate=0.001):\n",
        "    \"\"\"è¨­å®šè¨“ç·´ç›¸é—œçš„æå¤±å‡½æ•¸ã€å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨\"\"\"\n",
        "\n",
        "    # æå¤±å‡½æ•¸ - ä½¿ç”¨äº¤å‰ç†µæå¤±ï¼Œä¸¦åŠ å…¥æ¨™ç±¤å¹³æ»‘\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # å„ªåŒ–å™¨ - ä½¿ç”¨AdamWï¼ŒåŠ å…¥æ¬Šé‡è¡°æ¸›\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    # å­¸ç¿’ç‡èª¿åº¦å™¨ - ä½¿ç”¨é¤˜å¼¦é€€ç«\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=10,  # ç¬¬ä¸€æ¬¡é‡å•Ÿçš„é€±æœŸ\n",
        "        T_mult=2,  # æ¯æ¬¡é‡å•Ÿå¾Œé€±æœŸçš„å€æ•¸\n",
        "        eta_min=1e-6  # æœ€å°å­¸ç¿’ç‡\n",
        "    )\n",
        "\n",
        "    return criterion, optimizer, scheduler\n",
        "\n",
        "# è¨“ç·´å‡½æ•¸\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"è¨“ç·´ä¸€å€‹epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc='Training')\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        # é›¶æ¢¯åº¦\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # å‰å‘å‚³æ’­\n",
        "        with torch.cuda.amp.autocast():  # ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # åå‘å‚³æ’­\n",
        "        loss.backward()\n",
        "\n",
        "        # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # å„ªåŒ–å™¨æ­¥é€²\n",
        "        optimizer.step()\n",
        "\n",
        "        # çµ±è¨ˆ\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "        # æ›´æ–°é€²åº¦æ¢\n",
        "        current_acc = running_corrects.double() / total_samples\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{running_loss / total_samples:.4f}',\n",
        "            'Acc': f'{current_acc:.4f}'\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "# é©—è­‰å‡½æ•¸\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"é©—è­‰ä¸€å€‹epoch\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc='Validation')\n",
        "\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # å‰å‘å‚³æ’­\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # çµ±è¨ˆ\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "            # æ”¶é›†é æ¸¬çµæœç”¨æ–¼æ··æ·†çŸ©é™£\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # æ›´æ–°é€²åº¦æ¢\n",
        "            current_acc = running_corrects.double() / total_samples\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{running_loss / total_samples:.4f}',\n",
        "                'Acc': f'{current_acc:.4f}'\n",
        "            })\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "    return epoch_loss, epoch_acc.item(), all_preds, all_labels\n",
        "\n",
        "# ä¸»è¨“ç·´å¾ªç’°\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                num_epochs=50, patience=10, target_accuracy=0.95):\n",
        "    \"\"\"å®Œæ•´çš„è¨“ç·´éç¨‹\"\"\"\n",
        "\n",
        "    print(f\"é–‹å§‹è¨“ç·´ï¼Œç›®æ¨™æº–ç¢ºç‡: {target_accuracy:.1%}\")\n",
        "    print(f\"æœ€å¤§è¨“ç·´è¼ªæ•¸: {num_epochs}\")\n",
        "    print(f\"æ—©åœè€å¿ƒå€¼: {patience}\")\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = None\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # è¨“ç·´éšæ®µ\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # é©—è­‰éšæ®µ\n",
        "        val_loss, val_acc, val_preds, val_labels = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        # å­¸ç¿’ç‡èª¿åº¦\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # è¨˜éŒ„çµæœ\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f'è¨“ç·´ - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}')\n",
        "        print(f'é©—è­‰ - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}')\n",
        "        print(f'å­¸ç¿’ç‡: {current_lr:.6f}')\n",
        "\n",
        "        # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™æº–ç¢ºç‡\n",
        "        if val_acc >= target_accuracy:\n",
        "            print(f'\\nğŸ‰ é”åˆ°ç›®æ¨™æº–ç¢ºç‡ {target_accuracy:.1%}ï¼')\n",
        "            best_model_wts = model.state_dict().copy()\n",
        "            best_acc = val_acc\n",
        "            break\n",
        "\n",
        "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "            print(f'ğŸ’¾ æ–°çš„æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_acc:.4f}')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f'é©—è­‰æº–ç¢ºç‡æœªæå‡ ({patience_counter}/{patience})')\n",
        "\n",
        "        # æ—©åœæª¢æŸ¥\n",
        "        if patience_counter >= patience:\n",
        "            print(f'\\næ—©åœè§¸ç™¼ï¼æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_acc:.4f}')\n",
        "            break\n",
        "\n",
        "    # è¼‰å…¥æœ€ä½³æ¨¡å‹æ¬Šé‡\n",
        "    if best_model_wts is not None:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # è¿”å›è¨“ç·´æ­·å²\n",
        "    history = {\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accs': val_accs,\n",
        "        'best_acc': best_acc\n",
        "    }\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# è¨­å®šè¨“ç·´åƒæ•¸\n",
        "criterion, optimizer, scheduler = setup_training(model, learning_rate=0.001)\n",
        "\n",
        "print(\"è¨“ç·´è¨­å®šå®Œæˆï¼\")\n",
        "print(f\"æå¤±å‡½æ•¸: {criterion}\")\n",
        "print(f\"å„ªåŒ–å™¨: {optimizer}\")\n",
        "print(f\"å­¸ç¿’ç‡èª¿åº¦å™¨: {scheduler}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNPp_vT48xes",
        "outputId": "b843099c-28bb-443f-919f-a30a4ed27edd"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 6: æ¨¡å‹è¨“ç·´åŸ·è¡Œ\n",
        "# ==========================================\n",
        "\n",
        "# é–‹å§‹è¨“ç·´æ¨¡å‹\n",
        "print(\"é–‹å§‹è¨“ç·´æ¨¡å‹...\")\n",
        "print(f\"ç›®æ¨™é¡åˆ¥: {TARGET_CATEGORIES}\")\n",
        "print(f\"è¨“ç·´æ¨£æœ¬æ•¸: {len(train_data)}\")\n",
        "print(f\"é©—è­‰æ¨£æœ¬æ•¸: {len(val_data)}\")\n",
        "print(f\"ä½¿ç”¨è£ç½®: {device}\")\n",
        "\n",
        "# åŸ·è¡Œè¨“ç·´\n",
        "trained_model, training_history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=30,  # æœ€å¤§è¨“ç·´è¼ªæ•¸\n",
        "    patience=10,    # æ—©åœè€å¿ƒå€¼\n",
        "    target_accuracy=0.95  # ç›®æ¨™æº–ç¢ºç‡ 95%\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ¯ è¨“ç·´å®Œæˆï¼\")\n",
        "print(f\"æœ€ä½³é©—è­‰æº–ç¢ºç‡: {training_history['best_acc']:.4f}\")\n",
        "\n",
        "# ä¿å­˜æ¨¡å‹\n",
        "model_save_path = '/content/best_efficientnet_model.pth'\n",
        "torch.save({\n",
        "    'model_state_dict': trained_model.state_dict(),\n",
        "    'model_architecture': 'efficientnet_b3',\n",
        "    'num_classes': 4,\n",
        "    'target_categories': TARGET_CATEGORIES,\n",
        "    'training_history': training_history,\n",
        "    'feature_dim': trained_model.feature_dim\n",
        "}, model_save_path)\n",
        "\n",
        "print(f\"æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}\")\n",
        "\n",
        "# é¡¯ç¤ºè¨“ç·´æ­·å²æ‘˜è¦\n",
        "print(f\"\\nğŸ“Š è¨“ç·´æ­·å²æ‘˜è¦:\")\n",
        "print(f\"è¨“ç·´è¼ªæ•¸: {len(training_history['train_losses'])}\")\n",
        "print(f\"æœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {training_history['train_accs'][-1]:.4f}\")\n",
        "print(f\"æœ€çµ‚é©—è­‰æº–ç¢ºç‡: {training_history['val_accs'][-1]:.4f}\")\n",
        "print(f\"æœ€ä½³é©—è­‰æº–ç¢ºç‡: {training_history['best_acc']:.4f}\")\n",
        "\n",
        "# å¦‚æœéœ€è¦ï¼Œå¯ä»¥é€²è¡Œå¾®èª¿è¨“ç·´\n",
        "def fine_tune_training(model, train_loader, val_loader, target_accuracy=0.95):\n",
        "    \"\"\"å¾®èª¿è¨“ç·´ - è§£å‡æ›´å¤šå±¤é€²è¡Œç²¾ç´°èª¿æ•´\"\"\"\n",
        "    print(\"\\nğŸ”§ é–‹å§‹å¾®èª¿è¨“ç·´...\")\n",
        "\n",
        "    # è§£å‡backboneçš„æœ€å¾Œå¹¾å±¤\n",
        "    model.unfreeze_last_layers(num_layers=10)\n",
        "\n",
        "    # é™ä½å­¸ç¿’ç‡é€²è¡Œå¾®èª¿\n",
        "    fine_tune_optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=0.0001,  # æ›´å°çš„å­¸ç¿’ç‡\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    fine_tune_scheduler = optim.lr_scheduler.StepLR(\n",
        "        fine_tune_optimizer,\n",
        "        step_size=5,\n",
        "        gamma=0.5\n",
        "    )\n",
        "\n",
        "    # å¾®èª¿è¨“ç·´\n",
        "    fine_tuned_model, fine_tune_history = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=fine_tune_optimizer,\n",
        "        scheduler=fine_tune_scheduler,\n",
        "        num_epochs=20,  # è¼ƒå°‘çš„è¨“ç·´è¼ªæ•¸\n",
        "        patience=5,     # è¼ƒå°çš„è€å¿ƒå€¼\n",
        "        target_accuracy=target_accuracy\n",
        "    )\n",
        "\n",
        "    return fine_tuned_model, fine_tune_history\n",
        "\n",
        "# æª¢æŸ¥æ˜¯å¦éœ€è¦å¾®èª¿\n",
        "# if training_history['best_acc'] < 0.95:\n",
        "#     print(f\"\\nç•¶å‰æœ€ä½³æº–ç¢ºç‡ {training_history['best_acc']:.4f} < 95%ï¼Œé–‹å§‹å¾®èª¿...\")\n",
        "#     trained_model, fine_tune_history = fine_tune_training(\n",
        "#         trained_model, train_loader, val_loader, target_accuracy=0.95\n",
        "#     )\n",
        "\n",
        "#     # æ›´æ–°è¨“ç·´æ­·å²\n",
        "#     training_history['fine_tune_history'] = fine_tune_history\n",
        "#     training_history['final_best_acc'] = fine_tune_history['best_acc']\n",
        "\n",
        "#     # é‡æ–°ä¿å­˜æ¨¡å‹\n",
        "#     torch.save({\n",
        "#         'model_state_dict': trained_model.state_dict(),\n",
        "#         'model_architecture': 'efficientnet_b3',\n",
        "#         'num_classes': 4,\n",
        "#         'target_categories': TARGET_CATEGORIES,\n",
        "#         'training_history': training_history,\n",
        "#         'feature_dim': trained_model.feature_dim\n",
        "#     }, model_save_path)\n",
        "\n",
        "#     print(f\"å¾®èª¿å¾Œæœ€ä½³æº–ç¢ºç‡: {fine_tune_history['best_acc']:.4f}\")\n",
        "\n",
        "# print(\"\\nâœ… æ¨¡å‹è¨“ç·´éšæ®µå®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dD_tESnk-C5O",
        "outputId": "dda91e7f-cbaf-43ce-c417-860f063024f9"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Part 7: æ¨¡å‹è©•ä¼°èˆ‡æ··æ·†çŸ©é™£\n",
        "# ==========================================\n",
        "\n",
        "def evaluate_model(model, data_loader, device, class_names):\n",
        "    \"\"\"\n",
        "    è¨ˆç®—æ•´é«”æº–ç¢ºç‡ã€å„é¡åˆ¥æº–ç¢ºç‡å’Œæ··æ·†çŸ©é™£\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    print(\"æ­£åœ¨é€²è¡Œæ¨¡å‹è©•ä¼°...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc='è©•ä¼°ä¸­'):\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # å‰å‘å‚³æ’­\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # æ”¶é›†çµæœ\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # è½‰æ›ç‚ºnumpyé™£åˆ—\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    return all_preds, all_labels, all_probs\n",
        "\n",
        "def plot_confusion_matrices(y_true, y_pred, class_names):\n",
        "    \"\"\"\n",
        "    ç¹ªè£½æ¯å€‹é¡åˆ¥çš„æ··æ·†çŸ©é™£å’Œæ•´é«”æ··æ·†çŸ©é™£\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é™£...\")\n",
        "\n",
        "    # æ•´é«”æ··æ·†çŸ©é™£\n",
        "    overall_cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # è¨­å®šåœ–è¡¨æ¨£å¼\n",
        "    plt.style.use('default')\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('COCO 4-Class Classification - Confusion Matrices', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # é¡è‰²æ˜ å°„\n",
        "    colors = ['Blues', 'Greens', 'Oranges', 'Reds', 'Purples']\n",
        "\n",
        "    # ç¹ªè£½æ¯å€‹é¡åˆ¥çš„æ··æ·†çŸ©é™£\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        row = i // 3\n",
        "        col = i % 3\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        # ç‚ºç•¶å‰é¡åˆ¥å‰µå»ºäºŒå…ƒæ··æ·†çŸ©é™£\n",
        "        # True Positive, False Negative, False Positive, True Negative\n",
        "        y_true_binary = (y_true == i).astype(int)\n",
        "        y_pred_binary = (y_pred == i).astype(int)\n",
        "\n",
        "        binary_cm = confusion_matrix(y_true_binary, y_pred_binary)\n",
        "\n",
        "        # ç¹ªè£½ç†±åœ–\n",
        "        sns.heatmap(binary_cm,\n",
        "                   annot=True,\n",
        "                   fmt='d',\n",
        "                   cmap=colors[i],\n",
        "                   xticklabels=[f'Not {class_name}', f'{class_name}'],\n",
        "                   yticklabels=[f'Not {class_name}', f'{class_name}'],\n",
        "                   ax=ax,\n",
        "                   cbar_kws={'shrink': 0.8})\n",
        "\n",
        "        # è¨ˆç®—è©²é¡åˆ¥çš„ç²¾ç¢ºç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•¸\n",
        "        TP = binary_cm[1, 1]\n",
        "        FN = binary_cm[1, 0]\n",
        "        FP = binary_cm[0, 1]\n",
        "        TN = binary_cm[0, 0]\n",
        "\n",
        "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "        ax.set_title(f'{class_name}\\nAcc: {accuracy:.3f} | P: {precision:.3f} | R: {recall:.3f} | F1: {f1:.3f}')\n",
        "        ax.set_xlabel('Predicted')\n",
        "        ax.set_ylabel('Actual')\n",
        "\n",
        "    # ç¹ªè£½æ•´é«”æ··æ·†çŸ©é™£\n",
        "    ax_overall = axes[1, 1]\n",
        "    sns.heatmap(overall_cm,\n",
        "               annot=True,\n",
        "               fmt='d',\n",
        "               cmap='viridis',\n",
        "               xticklabels=class_names,\n",
        "               yticklabels=class_names,\n",
        "               ax=ax_overall,\n",
        "               cbar_kws={'shrink': 0.8})\n",
        "\n",
        "    ax_overall.set_title('Overall Confusion Matrix')\n",
        "    ax_overall.set_xlabel('Predicted')\n",
        "    ax_overall.set_ylabel('Actual')\n",
        "\n",
        "    # éš±è—æœ€å¾Œä¸€å€‹å­åœ–\n",
        "    axes[1, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return overall_cm\n",
        "\n",
        "def calculate_detailed_metrics(y_true, y_pred, y_probs, class_names):\n",
        "    \"\"\"\n",
        "    è¨ˆç®—è©³ç´°çš„è©•ä¼°æŒ‡æ¨™\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ“ˆ è¨ˆç®—è©³ç´°è©•ä¼°æŒ‡æ¨™...\")\n",
        "\n",
        "    # æ•´é«”æº–ç¢ºç‡\n",
        "    overall_accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # åˆ†é¡å ±å‘Š\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "    print(f\"\\nğŸ¯ æ•´é«”æº–ç¢ºç‡: {overall_accuracy:.4f} ({overall_accuracy:.1%})\")\n",
        "    print(\"\\nğŸ“‹ å„é¡åˆ¥è©³ç´°æŒ‡æ¨™:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'é¡åˆ¥':<12} {'ç²¾ç¢ºç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•¸':<10} {'æº–ç¢ºç‡':<10} {'æ”¯æŒæ•¸':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # è¨ˆç®—å„é¡åˆ¥çš„æº–ç¢ºç‡\n",
        "    class_accuracies = []\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        y_true_binary = (y_true == i).astype(int)\n",
        "        y_pred_binary = (y_pred == i).astype(int)\n",
        "\n",
        "        # è¨ˆç®—æ··æ·†çŸ©é™£å…ƒç´ \n",
        "        TP = np.sum((y_true_binary == 1) & (y_pred_binary == 1))\n",
        "        TN = np.sum((y_true_binary == 0) & (y_pred_binary == 0))\n",
        "        FP = np.sum((y_true_binary == 0) & (y_pred_binary == 1))\n",
        "        FN = np.sum((y_true_binary == 1) & (y_pred_binary == 0))\n",
        "\n",
        "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "        class_accuracies.append(accuracy)\n",
        "\n",
        "        precision = report[class_name]['precision']\n",
        "        recall = report[class_name]['recall']\n",
        "        f1 = report[class_name]['f1-score']\n",
        "        support = report[class_name]['support']\n",
        "\n",
        "        print(f\"{class_name:<12} {precision:<10.3f} {recall:<10.3f} {f1:<10.3f} {accuracy:<10.3f} {support:<10.0f}\")\n",
        "\n",
        "        # æª¢æŸ¥æ˜¯å¦é”åˆ°95%æº–ç¢ºç‡\n",
        "        if accuracy >= 0.95:\n",
        "            print(f\"  âœ… {class_name} é”åˆ°95%æº–ç¢ºç‡ç›®æ¨™ï¼\")\n",
        "        else:\n",
        "            print(f\"  âŒ {class_name} æœªé”åˆ°95%æº–ç¢ºç‡ç›®æ¨™ (ç›®å‰: {accuracy:.1%})\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # çµ±è¨ˆé”åˆ°95%æº–ç¢ºç‡çš„é¡åˆ¥æ•¸\n",
        "    classes_above_95 = sum(1 for acc in class_accuracies if acc >= 0.95)\n",
        "\n",
        "    print(f\"\\nğŸ“Š é”åˆ°95%æº–ç¢ºç‡çš„é¡åˆ¥æ•¸: {classes_above_95}/{len(class_names)}\")\n",
        "\n",
        "    if classes_above_95 == len(class_names):\n",
        "        print(\"ğŸ‰ æ‰€æœ‰é¡åˆ¥éƒ½é”åˆ°äº†95%æº–ç¢ºç‡ç›®æ¨™ï¼\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  é‚„æœ‰ {len(class_names) - classes_above_95} å€‹é¡åˆ¥æœªé”åˆ°95%æº–ç¢ºç‡ç›®æ¨™\")\n",
        "\n",
        "    return {\n",
        "        'overall_accuracy': overall_accuracy,\n",
        "        'class_accuracies': dict(zip(class_names, class_accuracies)),\n",
        "        'classification_report': report,\n",
        "        'classes_above_95_percent': classes_above_95\n",
        "    }\n",
        "\n",
        "# åŸ·è¡Œæ¨¡å‹è©•ä¼°\n",
        "print(\"é–‹å§‹è©•ä¼°è¨“ç·´å¥½çš„æ¨¡å‹...\")\n",
        "\n",
        "# åœ¨é©—è­‰é›†ä¸Šè©•ä¼°\n",
        "test_preds, test_labels, test_probs = evaluate_model(trained_model, test_loader, device, TARGET_CATEGORIES)\n",
        "\n",
        "# ç¹ªè£½æ··æ·†çŸ©é™£\n",
        "overall_cm = plot_confusion_matrices(test_labels, test_preds, TARGET_CATEGORIES)\n",
        "\n",
        "# è¨ˆç®—è©³ç´°æŒ‡æ¨™\n",
        "detailed_metrics = calculate_detailed_metrics(test_labels, test_preds, test_probs, TARGET_CATEGORIES)\n",
        "\n",
        "# ä¿å­˜è©•ä¼°çµæœ\n",
        "evaluation_results = {\n",
        "    'predictions': test_preds.tolist(),\n",
        "    'true_labels': test_labels.tolist(),\n",
        "    'probabilities': test_probs.tolist(),\n",
        "    'confusion_matrix': overall_cm.tolist(),\n",
        "    'detailed_metrics': detailed_metrics,\n",
        "    'class_names': TARGET_CATEGORIES\n",
        "}\n",
        "\n",
        "# ä¿å­˜åˆ°æ–‡ä»¶\n",
        "import json\n",
        "with open('/content/evaluation_results.json', 'w') as f:\n",
        "    json.dump(evaluation_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nğŸ’¾ è©•ä¼°çµæœå·²ä¿å­˜è‡³: /content/evaluation_results.json\")\n",
        "print(\"\\nâœ… æ¨¡å‹è©•ä¼°å®Œæˆï¼\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
